services:
  sql-client:
    image: rrchak/cpflinklocal
    ports:
      - "8084:8083"
    depends_on:
      - jobmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        rest.address: jobmanager
        flink.hadoop.fs.s3a.access.key: admin
        flink.hadoop.fs.s3a.secret.key: password
        flink.hadoop.fs.s3a.endpoint: http://minio:9000
        flink.hadoop.fs.s3a.path.style.access: true  
        #Uncomment this when using hive meta store     
        fs.s3a.access.key: admin
        fs.s3a.secret.key: password
        fs.s3a.endpoint: http://minio:9000
        fs.s3a.path.style.access: true 
        #s3.access.key: admin
        #s3.secret.key: password
        #s3.path.style.access: true
        #s3.endpoint: http://localhost:9000  
      - ROOT_LOG_LEVEL=TRACE
    networks:
       flinkiceberg:      
  jobmanager:
    image: rrchak/cpflinklocal
    hostname: jobmanager
    container_name: jobmanager
    ports:
      - "0.0.0.0:8082:8081"
    command: jobmanager
    volumes:
      - .:/data/
      #- ./log4j.properties:/opt/flink/conf/log4j.properties  
    environment:
    - |
      FLINK_PROPERTIES=
      jobmanager.rpc.address: jobmanager
      #jobmanager.rpc.address: 0.0.0.0  
      jobmanager.bind-host: 0.0.0.0
      rest.bind-address: 0.0.0.0
      rest.address: 0.0.0.0 
      rest.bind-port: 8081
      rest.flamegraph.enabled: true
      flink.hadoop.fs.s3a.access.key: admin
      flink.hadoop.fs.s3a.secret.key: password
      flink.hadoop.fs.s3a.endpoint: http://minio:9000
      flink.hadoop.fs.s3a.path.style.access: true  
      fs.s3a.access.key: admin
      fs.s3a.secret.key: password
      fs.s3a.endpoint: http://minio:9000
      fs.s3a.path.style.access: true 
      #s3.access.key: admin
      #s3.secret.key: password
      #s3.path.style.access: true
      #s3.endpoint: http://localhost:9000
      execution.checkpointing.interval: 60000
      execution.checkpointing.mode: EXACTLY_ONCE  
    - ROOT_LOG_LEVEL=TRACE
    networks:
       flinkiceberg:  
  taskmanager:
    image: rrchak/cpflinklocal
    hostname: taskmanager
    depends_on:
      - jobmanager
    command: taskmanager
  #volumes:
  #    - ./log4j.properties:/opt/flink/conf/log4j.properties   
    deploy:
      replicas: 2
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 4
        flink.hadoop.fs.s3a.access.key: admin
        flink.hadoop.fs.s3a.secret.key: password
        flink.hadoop.fs.s3a.endpoint: http://minio:9000
        flink.hadoop.fs.s3a.path.style.access: true  
        fs.s3a.access.key: admin
        fs.s3a.secret.key: password
        fs.s3a.endpoint: http://minio:9000
        fs.s3a.path.style.access: true
        #s3.access.key: admin
        #s3.secret.key: password
        #s3.endpoint: http://localhost:9000
        #s3.path.style.access: true  
        execution.checkpointing.interval: 60000
        execution.checkpointing.mode: EXACTLY_ONCE  
      - ROOT_LOG_LEVEL=TRACE
     #- AWS_ACCESS_KEY_ID=admin
     #- AWS_SECRET_ACCESS_KEY=password
     #- AWS_REGION=us-east-1  
    networks:
       flinkiceberg:    
  hms:
    #image: ghcr.io/recap-build/hive-metastore-standalone:latest
    image: rrchak/hms 
    hostname: hms
    container_name: hms
    ports:
      - "9083:9083" 
    networks:
       flinkiceberg:   
         aliases:
           - hms 
    #Any Iceberg runtime may be connected to them, and any Iceberg-compatible processing engine can use them to load the tracked Iceberg tables.
  rest:
    image: tabulario/iceberg-rest
    #image: apache/iceberg-rest-fixture  
    container_name: iceberg-rest
    hostname: rest   
    ports:
      - 8185:8181
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
      - CATALOG_WAREHOUSE=s3a://warehouse/
      - CATALOG_IO__IMPL=org.apache.iceberg.aws.s3.S3FileIO
      - CATALOG_S3_ENDPOINT=http://minio:9000
    networks:
       flinkiceberg:    
  control-center:
    image: confluentinc/cp-enterprise-control-center:7.9.0
    hostname: control-center
    container_name: control-center
    ports:
      - "9021:9021"
    depends_on:
      - connect    
    volumes:    
      - ./connectcerts:/etc/kafka/secrets
    environment:
      CONTROL_CENTER_BOOTSTRAP_SERVERS: "xxx-yyy.us-east1.gcp.confluent.cloud:9092"
      CONTROL_CENTER_SCHEMA_REGISTRY_URL: "https://xxx-yyy.us-east-2.aws.confluent.cloud"
      CONTROL_CENTER_SCHEMA_REGISTRY_BASIC_AUTH_CREDENTIALS_SOURCE: "USER_INFO"  
      CONTROL_CENTER_SCHEMA_REGISTRY_BASIC_AUTH_USER_INFO: "someapikey:someapisecret"   
      CONTROL_CENTER_CONNECT_CONNECT1_CLUSTER: "https://connect:8083"
      CONTROL_CENTER_REST_PROXY_SSL_TRUSTSTORE_LOCATION: /etc/kafka/secrets/kafka.truststore.jks
      CONTROL_CENTER_REST_PROXY_SSL_TRUSTSTORE_PASSWORD: confluent  
      CONTROL_CENTER_CONNECT_CONNECT1_SSL_TRUSTSTORE_LOCATION: /etc/kafka/secrets/kafka.truststore.jks
      CONTROL_CENTER_CONNECT_CONNECT1_SSL_TRUSTSTORE_PASSWORD: confluent
      CONTROL_CENTER_CONNECT_CONNECT1_SSL_KEYSTORE_LOCATION: /etc/kafka/secrets/kafka.keystore.jks
      CONTROL_CENTER_CONNECT_CONNECT1_SSL_KEYSTORE_PASSWORD: confluent
      CONTROL_CENTER_CONNECT_CONNECT1_SSL_KEY_PASSWORD: confluent 
      CONTROL_CENTER_STREAMS_SECURITY_PROTOCOL: SASL_SSL
      CONTROL_CENTER_STREAMS_SASL_JAAS_CONFIG: "org.apache.kafka.common.security.plain.PlainLoginModule required \
                username=\"someuser\" \
                password=\"xxx/yyy\";" 
      CONTROL_CENTER_STREAMS_SASL_MECHANISM: PLAIN
      CONTROL_CENTER_MODE_ENABLE: "management"
      CONTROL_CENTER_REPLICATION_FACTOR: 3
      PORT: 9021     
    networks:
       flinkiceberg:    
  #spark-iceberg:
  #  image: tabulario/spark-iceberg
  #  container_name: spark-iceberg
  #  build: spark/
  #  command: [ "/bin/bash", "-c", "--",  "jupyter", "notebook", "--ip 0.0.0.0", "--no-browser", "--allow-root" ]  
  #  depends_on:
  #    - rest
  #    - minio
  #  volumes:
  #    - ./warehouse:/home/iceberg/warehouse
  #    - ./notebooks:/home/iceberg/notebooks/notebooks
  #  environment:
  #    - AWS_ACCESS_KEY_ID=admin
  #    - AWS_SECRET_ACCESS_KEY=password
  #    - AWS_REGION=us-east-1
  #  ports:
  #    - 8888:8888
  #    - 8080:8080
  #    - 10000:10000
  #    - 10001:10001      
  #  networks:
  #    flinkiceberg:    
  spark-iceberg-master:
    #image: tabulario/spark-iceberg
    image: rrchak/sparklocal
    container_name: spark-iceberg-master
    build: spark/
    #command: >
    #  jupyter notebook --ip 0.0.0.0 --port 8443 --no-browser --allow-root 
    #command: [ "/bin/sh", "-c", "--",  "jupyter", "notebook", "--ip 0.0.0.0", "--no-browser", "--allow-root" ]  
    depends_on:
      - rest
      - minio
    volumes:
      - /home/ubuntu/cpflink/log4j.properties/log4j2.properties:/opt/spark/conf/log4j2.properties
      - ./warehouse:/home/iceberg/warehouse
      - ./notebooks:/home/iceberg/notebooks/notebooks
      - ./spark-delta-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./jupyter_notebook_config.py:/root/.jupyter/jupyter_notebook_config.py
      - ./ssl/server.crt:/root/.jupyter/server.crt
      - ./ssl/server.key:/root/.jupyter/server.key
      - ./logs:/opt/spark/logs/
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
      - SPARK_MODE=master
      - SPARK_LOCAL_IP=spark-master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - CC_ACCESS_KEY="xxx"
      - CC_SECRET="yyy"
      - SR_ACCESS_KEY="zzz"
      - SR_SECRET="aaa"
      - CC_URL="bbb"
      - SR_URL="ccc"
    ports:
      - 7077:7077
      - 8888:8888
      - 8443:8443
      - 8080:8080
      - 10000:10000
      - 10001:10001
    networks:
      flinkiceberg:
        aliases:
          - spark-master
  spark-worker-1:
    image: rrchak/sparklocal
    container_name: "spark-iceberg-worker-1"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-iceberg-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - CC_ACCESS_KEY="xxx"
      - CC_SECRET="yyy"
      - SR_ACCESS_KEY="zzz"
      - SR_SECRET="aaa"
      - CC_URL="bbb"
      - SR_URL="ccc"
    networks:
       flinkiceberg:         
  mysql:
    build:
      context: .
      dockerfile: Dockerfile-mysql
   #image: mysql-shm:5.6.38
    container_name: mysql-shm
    environment:
      - MYSQL_ROOT_PASSWORD=mysecret
      - MYSQL_ROOT_HOST= '%'
    ports:
     - "3306:3306"
    volumes:
     - $PWD/container_data/mysql:/var/lib/mysql        
    networks:
       flinkiceberg:   
  connect:
    image: rrchak/ccconnect 
    hostname: connect
    container_name: connect
    ports:
      - "8083:8083"
      - "9101:9101"
      - "9404:9404"
    volumes:
      - ./jmx_prometheus_javaagent-0.12.0.jar:/usr/share/java/kafka/jmx_prometheus_javaagent-0.12.0.jar    
      - ./kafka_connect.yml:/usr/share/java/kafka/kafka_connect.yml  
      - ./connectcerts:/etc/kafka/secrets  
    environment:
      CONNECT_LISTENERS: "HTTPS://0.0.0.0:8083"
      CONNECT_REST_ADVERTISED_HOST_NAME: "connect"
      CONNECT_REST_ADVERTISED_PORT: "8083"
      CONNECT_REST_PORT: "8083"
      CONNECT_REST_ADVERTISED_LISTENER: https   
      CONNECT_BOOTSTRAP_SERVERS: xxx-yyy.us-east1.gcp.confluent.cloud:9092
      CONNECT_GROUP_ID: "connect"
      CONNECT_CONFIG_STORAGE_TOPIC: demo-connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: demo-connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: demo-connect-status
      CONNECT_REPLICATION_FACTOR: 3
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.storage.StringConverter"
      CONNECT_VALUE_CONVERTER: "io.confluent.connect.avro.AvroConverter"
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "true"
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: https://xxx-yyy.us-east-2.aws.confluent.cloud
      CONNECT_VALUE_CONVERTER_BASIC_AUTH_CREDENTIALS_SOURCE: USER_INFO
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_BASIC_AUTH_USER_INFO: "someapikey:someapisecret" 
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
      CONNECT_LOG4J_ROOT_LOGLEVEL: INFO
      CONNECT_LOG4J_LOGGERS: org.reflections=ERROR
      # CLASSPATH required due to CC-2422
      CLASSPATH: /usr/share/java/monitoring-interceptors/monitoring-interceptors-6.2.0.jar
      # Connect worker
      CONNECT_SECURITY_PROTOCOL: SASL_SSL
      CONNECT_SASL_JAAS_CONFIG: "org.apache.kafka.common.security.plain.PlainLoginModule required \
                username=\"someuser\" \
                password=\"xxx/yyy\";"
      CONNECT_SASL_MECHANISM: PLAIN
      # Connect producer
      CONNECT_PRODUCER_SECURITY_PROTOCOL: SASL_SSL
      CONNECT_PRODUCER_SASL_JAAS_CONFIG: "org.apache.kafka.common.security.plain.PlainLoginModule required \
                username=\"someuser\" \
                password=\"xxx/yyy\";"
      CONNECT_PRODUCER_SASL_MECHANISM: PLAIN
      CONNECT_PRODUCER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor"
      CONNECT_PRODUCER_CONFLUENT_MONITORING_INTERCEPTOR_SECURITY_PROTOCOL: SASL_SSL
      CONNECT_PRODUCER_CONFLUENT_MONITORING_INTERCEPTOR_SASL_JAAS_CONFIG: "org.apache.kafka.common.security.plain.PlainLoginModule required \
                username=\"someuser\" \
                password=\"xxx/yyy\";"
      CONNECT_PRODUCER_CONFLUENT_MONITORING_INTERCEPTOR_SASL_MECHANISM: PLAIN
      # Connect consumer
      CONNECT_CONSUMER_SECURITY_PROTOCOL: SASL_SSL
      CONNECT_CONSUMER_SASL_JAAS_CONFIG: "org.apache.kafka.common.security.plain.PlainLoginModule required \
                username=\"someuser\" \
                password=\"xxx/yyy\";"
      CONNECT_CONSUMER_SASL_MECHANISM: PLAIN
      CONNECT_CONSUMER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor"
      CONNECT_CONSUMER_CONFLUENT_MONITORING_INTERCEPTOR_SECURITY_PROTOCOL: SASL_SSL
      CONNECT_CONSUMER_CONFLUENT_MONITORING_INTERCEPTOR_SASL_JAAS_CONFIG: "org.apache.kafka.common.security.plain.PlainLoginModule required \
                username=\"someuser\" \
                password=\"xxx/yyy\";"
      CONNECT_CONSUMER_CONFLUENT_MONITORING_INTERCEPTOR_SASL_MECHANISM: PLAIN        
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
      EXTRA_ARGS: -javaagent:/usr/share/java/kafka/jmx_prometheus_javaagent-0.12.0.jar=9404:/usr/share/java/kafka/kafka_connect.yml   
      #CONNECT_REST_ADVERTISED_LISTENER: "HTTPS://0.0.0.0:8083"
      CONNECT_LISTENERS_HTTPS_SSL_KEYSTORE_LOCATION: /etc/kafka/secrets/kafka.keystore.jks
      CONNECT_LISTENERS_HTTPS_SSL_KEYSTORE_PASSWORD: confluent
      CONNECT_LISTENERS_HTTPS_SSL_KEY_PASSWORD: confluent
      CONNECT_LISTENERS_HTTPS_SSL_TRUSTSTORE_LOCATION: /etc/kafka/secrets/kafka.truststore.jks
      CONNECT_LISTENERS_HTTPS_SSL_TRUSTSTORE_PASSWORD: confluent
      #CONNECT_SSL_KEYSTORE_TYPE: PEM
      #CONNECT_SSL_KEYSTORE_CERTIFICATE_CHAIN: "/etc/kafka/secrets/server.pem"
      #CONNECT_SSL_KEYSTORE_KEY: "/etc/kafka/secrets/serverkey.pem"
      #CONNECT_SSL_TRUSTSTORE_TYPE: PEM
      #CONNECT_SSL_TRUSTSTORE_LOCATION: "/etc/kafka/secrets/rootca.pem"
      #CONNECT_SSL_KEYSTORE_KEY_PASSWORD: ""
      #CONNECT_SSL_KEYSTORE_PASSWORD: ""    
    networks:
      flinkiceberg:
  minio:
    image: minio/minio
    hostname: minio   
    container_name: minio
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=password
      - MINIO_DOMAIN=minio  
    networks:
      flinkiceberg:
        aliases:
          - warehouse.minio     
    ports:
      - 9001:9001
      - 9000:9000
    command: ["server", "/data", "--console-address", ":9001"]
  mc:
    image: minio/mc
    hostname: mc  
    container_name: mc
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=password
      - CATALOG_WAREHOUSE_NAME=warehouse
    entrypoint: >
      /bin/sh -c "
      until (/usr/bin/mc config host add minio http://minio:9000 admin password) do echo '...waiting...' && sleep 1; done;
      /usr/bin/mc rm -r --force minio/warehouse;
      /usr/bin/mc mb minio/warehouse;
      tail -f /dev/null
      "     
    depends_on:
      - minio
    networks:
       flinkiceberg:
  dense-index:
    image: ghcr.io/pinecone-io/pinecone-local:latest
    container_name: dense-index
    environment:
      PORT: 5081
      PINECONE_HOST: localhost
      #INDEX_TYPE: serverless
      #VECTOR_TYPE: dense
      #DIMENSION: 2 
      #METRIC: cosine
    ports:
       - "5080-5090:5080-5090"
    platform: linux/amd64
    networks:
        flinkiceberg:
  
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.1
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
       flinkiceberg:    

  broker:
    image: confluentinc/cp-kafka:7.4.1
    hostname: broker
    container_name: broker
    depends_on:
      - zookeeper
    ports:
      - "29092:29092"
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
    networks:
       flinkiceberg:    
        
networks:
   flinkiceberg:      
     name: flinkiceberg
#networks:
#  default:
#     name: flinkiceberg       
       

          
